{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFl6kX-JeKdG"
   },
   "source": [
    "Установка необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bE1RDE13yXsw",
    "outputId": "f662d5af-1a83-4858-e98e-497c23c64397"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Double requirement given: matplotlib (already in matplotlib==3.4.3, name='matplotlib')\u001b[0m\n",
      "Requirement already satisfied: scikit-learn==0.24.2 in /home/genalll/.local/lib/python3.8/site-packages (0.24.2)\n",
      "Requirement already satisfied: torch==1.12.1 in /home/genalll/.local/lib/python3.8/site-packages (1.12.1)\n",
      "Requirement already satisfied: torchvision==0.13.1 in /home/genalll/.local/lib/python3.8/site-packages (0.13.1)\n",
      "Requirement already satisfied: pytorch-ignite in /home/genalll/.local/lib/python3.8/site-packages (0.4.10)\n",
      "Requirement already satisfied: segmentation-models-pytorch==0.2.0 in /home/genalll/.local/lib/python3.8/site-packages (0.2.0)\n",
      "Requirement already satisfied: albumentations==1.0.3 in /home/genalll/.local/lib/python3.8/site-packages (1.0.3)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/genalll/.local/lib/python3.8/site-packages (from scikit-learn==0.24.2) (1.9.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/genalll/.local/lib/python3.8/site-packages (from scikit-learn==0.24.2) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/genalll/.local/lib/python3.8/site-packages (from scikit-learn==0.24.2) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/genalll/.local/lib/python3.8/site-packages (from scikit-learn==0.24.2) (1.23.5)\n",
      "Requirement already satisfied: typing-extensions in /home/genalll/.local/lib/python3.8/site-packages (from torch==1.12.1) (4.4.0)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from torchvision==0.13.1) (2.22.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/lib/python3/dist-packages (from torchvision==0.13.1) (7.0.0)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3/dist-packages (from pytorch-ignite) (20.3)\n",
      "Requirement already satisfied: timm==0.4.12 in /home/genalll/.local/lib/python3.8/site-packages (from segmentation-models-pytorch==0.2.0) (0.4.12)\n",
      "Requirement already satisfied: pretrainedmodels==0.7.4 in /home/genalll/.local/lib/python3.8/site-packages (from segmentation-models-pytorch==0.2.0) (0.7.4)\n",
      "Requirement already satisfied: efficientnet-pytorch==0.6.3 in /home/genalll/.local/lib/python3.8/site-packages (from segmentation-models-pytorch==0.2.0) (0.6.3)\n",
      "Requirement already satisfied: PyYAML in /usr/lib/python3/dist-packages (from albumentations==1.0.3) (5.3.1)\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in /home/genalll/.local/lib/python3.8/site-packages (from albumentations==1.0.3) (4.6.0.66)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in /home/genalll/.local/lib/python3.8/site-packages (from albumentations==1.0.3) (0.19.3)\n",
      "Requirement already satisfied: tqdm in /home/genalll/.local/lib/python3.8/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch==0.2.0) (4.64.1)\n",
      "Requirement already satisfied: munch in /home/genalll/.local/lib/python3.8/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch==0.2.0) (2.5.0)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /home/genalll/.local/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations==1.0.3) (2.22.4)\n",
      "Requirement already satisfied: networkx>=2.2 in /home/genalll/.local/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations==1.0.3) (2.8.8)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /home/genalll/.local/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations==1.0.3) (1.4.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /home/genalll/.local/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations==1.0.3) (2022.10.10)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from munch->pretrainedmodels==0.7.4->segmentation-models-pytorch==0.2.0) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv==0.19.0 tqdm==4.62.2 numpy Pillow==7.0.0 matplotlib==3.4.3 opencv-python==4.6.0.66 opencv-python-headless==4.6.0.66 matplotlib\n",
    "!pip install scikit-learn==0.24.2 torch==1.12.1 torchvision==0.13.1 pytorch-ignite segmentation-models-pytorch==0.2.0 albumentations==1.0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WRhiSQ5zM8nO",
    "outputId": "53b64f66-6165-4abc-ae48-56bdde71edb0"
   },
   "source": [
    "!curl -o ~/plan-dataset.zip \"https://lodmedia.hb.bizmrg.com/cases/868821/masterclass.zip\"\n",
    "!unzip ~/plan-dataset.zip -d ~/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "B8hpHLWYv9Ak"
   },
   "outputs": [],
   "source": [
    "## Импорт необходимых библиотек\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Loss, Metric\n",
    "from ignite.engine import _prepare_batch\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "import base64\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Callable, Tuple, Dict, Any, List, Sequence, Iterator, Optional\n",
    "from collections import defaultdict\n",
    "from torchvision.models.detection.faster_rcnn import fasterrcnn_resnet50_fpn, FastRCNNPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGts2McXePcu"
   },
   "source": [
    "Код для обучения нейросети - сегментация "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LFuViSg26sA2",
    "outputId": "91e4b35a-8e62-49c5-985c-926b6ccdd72d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ITERATION - loss: 0.00:   0%|                          | 0/2450 [00:00<?, ?it/s]/home/genalll/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:173: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "ITERATION - loss: 0.13:  65%|█████████▊     | 1600/2450 [35:25<20:30,  1.45s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "## PLRC DATASET - класс и необходимые функции для датасета\n",
    "\n",
    "\n",
    "def get_color_map():\n",
    "    return {\n",
    "        \"wall\": 255,\n",
    "        \"window\": 255\n",
    "    }\n",
    "\n",
    "\n",
    "def tensor_from_rgb_image(image: np.ndarray) -> torch.Tensor:\n",
    "    image = np.moveaxis(image, -1, 0)\n",
    "    image = np.ascontiguousarray(image)\n",
    "    image = torch.from_numpy(image)\n",
    "    return image\n",
    "\n",
    "\n",
    "def tensor_from_mask_image(mask: np.ndarray) -> torch.Tensor:\n",
    "    if len(mask.shape) == 2:\n",
    "        mask = np.expand_dims(mask, -1)\n",
    "    return tensor_from_rgb_image(mask)\n",
    "\n",
    "\n",
    "class PLRCDataset(Dataset):\n",
    "    def __init__(self, image_folder, transform, start_index, end_index, mask_folder=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.mask_folder = mask_folder\n",
    "        self.transform = transform\n",
    "\n",
    "        self.images = PLRCDataset.parse_folder(self.image_folder, start_index, end_index)\n",
    "        self.color_map = get_color_map()\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_folder(path, start, end):\n",
    "        if path is None:\n",
    "            return []\n",
    "        images = glob.glob1(path,  '*.png')\n",
    "        images.sort()\n",
    "\n",
    "        return images[start:end]\n",
    "\n",
    "    @staticmethod\n",
    "    def load_image(path) -> np.array:\n",
    "        return cv2.imread(path, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_mask(path) -> np.array:\n",
    "        return cv2.imread(path, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def split_grayscale_mask_into_channels_by_color_map(mask, color_map) -> torch.Tensor:\n",
    "        masks = []\n",
    "\n",
    "        for i in color_map.values():\n",
    "            masks.append(mask == i)\n",
    "\n",
    "        return torch.cat(masks).float()\n",
    "\n",
    "    def mask_to_grayscale(self, masks) -> np.ndarray:\n",
    "        masks = masks.cpu().numpy()\n",
    "\n",
    "        colors_by_index = list(self.color_map.values())\n",
    "        img = np.zeros(masks.shape[1:], dtype=np.uint8)\n",
    "\n",
    "        for i in range(len(masks)):\n",
    "            img[masks[i] == 1] = colors_by_index[i]\n",
    "\n",
    "        return img\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_name = self.images[index]\n",
    "        image_path = os.path.join(self.image_folder, image_name)\n",
    "\n",
    "        image = PLRCDataset.load_image(image_path)\n",
    "\n",
    "        if self.mask_folder is None:\n",
    "            # sample = self.transform(image=image)\n",
    "            # image = sample['image']\n",
    "            return image_name, tensor_from_mask_image(image).float() / 255.0\n",
    "\n",
    "        mask_path = os.path.join(self.mask_folder, image_name)\n",
    "        mask = PLRCDataset.load_mask(mask_path)\n",
    "\n",
    "        # sample = self.transform(image=image, mask=mask)\n",
    "        # image, mask = sample['image'], sample['mask']\n",
    "\n",
    "        image = tensor_from_mask_image(image)\n",
    "        image = torch.cat([image, image, image])\n",
    "        mask = tensor_from_mask_image(mask)\n",
    "\n",
    "        mask = PLRCDataset.split_grayscale_mask_into_channels_by_color_map(mask, self.color_map)\n",
    "\n",
    "        return image.float() / 255.0, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "\n",
    "## PLRC UTILS = утилиты для подсчета\n",
    "\n",
    "\n",
    "def get_training_augmentation():\n",
    "    return A.Compose([\n",
    "        # A.RandomCrop(height=256, width=256, p=1),\n",
    "        A.Normalize(mean=(0.5,), std=(0.5,)),\n",
    "    ], p=1)\n",
    "\n",
    "\n",
    "def get_test_augmentation():\n",
    "    return A.Compose([\n",
    "        A.Normalize(mean=(0.5,), std=(0.5,)),\n",
    "    ], p=1)\n",
    "\n",
    "\n",
    "def get_data_loader(path, batch_size, n_processes, start_index, end_index, shuffle=True):\n",
    "    image_path = os.path.join(path, 'image')\n",
    "    mask_path = os.path.join(path, 'mask')\n",
    "\n",
    "    dataset = PLRCDataset(image_folder=image_path, mask_folder=mask_path, transform=get_training_augmentation(), start_index=start_index, end_index=end_index)\n",
    "\n",
    "    return DataLoader(dataset=dataset, batch_size=batch_size, drop_last=True, num_workers=n_processes, shuffle=shuffle)\n",
    "\n",
    "\n",
    "def get_train_validation_data_loaders(path, batch_size, n_processes, train_split):\n",
    "    files_count = len(os.listdir(os.path.join(path, 'image')))\n",
    "\n",
    "    train_dl = get_data_loader(path, batch_size, n_processes, shuffle=True, start_index=0, end_index=int(files_count*train_split))\n",
    "    test_dl = get_data_loader(path, batch_size, n_processes, shuffle=False, start_index=int(files_count*train_split), end_index=100)\n",
    "\n",
    "    return train_dl, test_dl\n",
    "\n",
    "\n",
    "## DATA LOSS - функции для подсчета метрик качества обучения нейросетей\n",
    "\n",
    "class BCESoftDiceLoss:\n",
    "    def __init__(self, dice_weight=0):\n",
    "        self.nll_loss = nn.BCEWithLogitsLoss()\n",
    "        self.dice_weight = dice_weight\n",
    "\n",
    "    @staticmethod\n",
    "    def soft_dice(predict, target):\n",
    "        eps = 1e-15\n",
    "        batch_size = target.size()[0]\n",
    "\n",
    "        dice_target = (target == 1).float().view(batch_size, -1)\n",
    "        dice_predict = torch.sigmoid(predict).view(batch_size, -1)\n",
    "\n",
    "        inter = torch.sum(dice_predict * dice_target) / batch_size\n",
    "        union = (torch.sum(dice_predict) + torch.sum(dice_target)) / batch_size + eps\n",
    "\n",
    "        return (2 * inter.float() + eps) / union.float()\n",
    "\n",
    "    def __call__(self, predict, target):\n",
    "        loss = (1.0 - self.dice_weight) * self.nll_loss(predict, target)\n",
    "\n",
    "        if self.dice_weight:\n",
    "            loss -= self.dice_weight * torch.log(BCESoftDiceLoss.soft_dice(predict, target))\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class MultiClassBCESoftDiceLoss:\n",
    "    def __init__(self, dice_weight=0):\n",
    "        self.bce_soft_dice = BCESoftDiceLoss(dice_weight)\n",
    "\n",
    "    def __call__(self, predict, target):\n",
    "        classes = target.shape[1]\n",
    "        loss = predict.new_zeros(1)\n",
    "\n",
    "        for i in range(classes):\n",
    "            loss += self.bce_soft_dice(predict[:, i].unsqueeze(1), target[:, i].unsqueeze(1))\n",
    "\n",
    "        return loss[0] / float(classes)\n",
    "\n",
    "\n",
    "class MultiClassSoftDiceMetric(Metric):\n",
    "    def __init__(self):\n",
    "        super(MultiClassSoftDiceMetric, self).__init__()\n",
    "        self.general_loss = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.general_loss = 0\n",
    "\n",
    "    def update(self, output):\n",
    "        predict, target = output\n",
    "\n",
    "        classes = target.shape[1]\n",
    "        loss = predict.new_zeros(1)\n",
    "\n",
    "        for i in range(classes):\n",
    "            loss += BCESoftDiceLoss.soft_dice(predict[:, i].unsqueeze(1), target[:, i].unsqueeze(1))\n",
    "\n",
    "        self.general_loss = loss[0] / float(classes)\n",
    "\n",
    "    def compute(self):\n",
    "        return self.general_loss\n",
    "\n",
    "\n",
    "\n",
    "## Функции для обучения\n",
    "\n",
    "def load_trained_model(model, optimizer, model_path, optimizer_path):\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    optimizer.load_state_dict(torch.load(optimizer_path))\n",
    "    print('Load model from: ', model_path)\n",
    "    print('Load optimizer from: ', optimizer_path)\n",
    "\n",
    "\n",
    "def save_model(model, optimizer, model_path, optimizer_path, postfix='_'):\n",
    "    torch.save(model.state_dict(), model_path + postfix)\n",
    "    torch.save(optimizer.state_dict(), optimizer_path + postfix)\n",
    "\n",
    "\n",
    "def log_image(image, prefix, epoch, step):\n",
    "    img = Image.fromarray(image)\n",
    "    image_name = \"%s_%s_%s.png\" % (epoch, step, prefix)\n",
    "    img.save(image_name)\n",
    "\n",
    "    os.remove(image_name)\n",
    "\n",
    "\n",
    "def run_test_model(model, evaluate_loader, epoch, device, step=10):\n",
    "    model.eval()\n",
    "    count_step = 0\n",
    "\n",
    "    for idx, batch in enumerate(evaluate_loader):\n",
    "        if count_step > step:\n",
    "            break\n",
    "\n",
    "        x, y = _prepare_batch(batch, device)\n",
    "\n",
    "        predict = model(x)\n",
    "        predict = torch.sigmoid(predict) > 0.2\n",
    "\n",
    "        count_step += len(x)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "\n",
    "def run_train(dataset_path, batch_size, n_processes, model_path, optimizer_path, load_pre_model=False,\n",
    "              device='cpu', lr=0.0001, betas=(0.9, 0.99), weight_decay=0.0004, epochs=10,\n",
    "              log_interval=20, save_interval=2, train_split=0.7):\n",
    "\n",
    "    train_loader, evaluate_loader = get_train_validation_data_loaders(path=dataset_path, batch_size=batch_size,\n",
    "                                                                      n_processes=n_processes, train_split=train_split)\n",
    "    model = smp.FPN('resnet50', classes=24)\n",
    "\n",
    "    if device.startswith('cuda'):\n",
    "        if not torch.cuda.is_available():\n",
    "            raise ValueError('CUDA is not available')\n",
    "\n",
    "        model = model.to(device)\n",
    "        print('CUDA is used')\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "\n",
    "    if load_pre_model:\n",
    "        load_trained_model(model, optimizer, model_path, optimizer_path)\n",
    "\n",
    "    trainer = create_supervised_trainer(model, optimizer, MultiClassBCESoftDiceLoss(0.7), device=device)\n",
    "    evaluator = create_supervised_evaluator(model,\n",
    "                                            metrics={'dice': MultiClassSoftDiceMetric(),\n",
    "                                                     'nll': Loss(MultiClassBCESoftDiceLoss(0.7))},\n",
    "                                            device=device)\n",
    "\n",
    "    desc = \"ITERATION - loss: {:.2f}\"\n",
    "    pbar = None\n",
    "\n",
    "    @trainer.on(Events.EPOCH_STARTED)\n",
    "    def create_pbar(engine):\n",
    "        model.train()\n",
    "        nonlocal pbar\n",
    "        pbar = tqdm(\n",
    "            initial=0, leave=False, total=len(train_loader),\n",
    "            desc=desc.format(0)\n",
    "        )\n",
    "\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def log_training_results(engine):\n",
    "        pbar.close()\n",
    "        evaluator.run(evaluate_loader)\n",
    "        metrics = evaluator.state.metrics\n",
    "        avg_dice = metrics['dice']\n",
    "        avg_nll = metrics['nll']\n",
    "\n",
    "\n",
    "        print(\"Training Results - Epoch: {}  Dice: {:.2f} Avg loss: {:.2f}\"\n",
    "              .format(engine.state.epoch, avg_dice, avg_nll))\n",
    "\n",
    "        if engine.state.epoch % save_interval == 0:\n",
    "            save_model(model, optimizer, model_path, optimizer_path, '_' + str(engine.state.epoch))\n",
    "            run_test_model(model, evaluate_loader, engine.state.epoch, device)\n",
    "\n",
    "    @trainer.on(Events.ITERATION_COMPLETED)\n",
    "    def log_training_loss(engine):\n",
    "\n",
    "        pbar.desc = desc.format(engine.state.output)\n",
    "        pbar.update()\n",
    "\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    trainer.run(train_loader, max_epochs=epochs)\n",
    "\n",
    "run_train(dataset_path=f\"{os.environ['HOME']}/train/wall\", batch_size=1, n_processes=0,\n",
    "          model_path='./model',\n",
    "          optimizer_path='./opt', device='cpu', epochs=1,\n",
    "          load_pre_model=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nV9nxL5wdar2"
   },
   "source": [
    "Код для получения данных с сегментации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "MrIseOYL6x94",
    "outputId": "4b571a28-ff0d-465c-c9f6-e180aa275235"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Необходимые вспомогательные функции\n",
    "def get_test_augmentation():\n",
    "    return A.Compose([\n",
    "        A.Normalize(mean=(0.5,), std=(0.5,)),\n",
    "    ], p=1)\n",
    "\n",
    "def mask_to_grayscale(masks) -> np.ndarray:\n",
    "    masks = masks.cpu().numpy()\n",
    "\n",
    "    img = np.zeros(masks.shape[1:], dtype=np.uint8)\n",
    "\n",
    "    img[masks[0] == 1] = 1\n",
    "\n",
    "    return img\n",
    "\n",
    "def tensor_from_rgb_image(image: np.ndarray) -> torch.Tensor:\n",
    "    image = np.moveaxis(image, -1, 0)\n",
    "    image = np.ascontiguousarray(image)\n",
    "    image = torch.from_numpy(image)\n",
    "    return image\n",
    "\n",
    "def convert_image(img):\n",
    "    if img is None:\n",
    "        return\n",
    "    height, width, channels = img.shape\n",
    "    rect = (0, 0, 512, 512)\n",
    "    h = rect[3]\n",
    "    w = int(h * width / height)\n",
    "    if w > rect[2]:\n",
    "        w = rect[2]\n",
    "        h = int(height / width * w)\n",
    "    new_img = cv2.resize(img, (w, h), interpolation=cv2.INTER_AREA)\n",
    "    s = np.full((rect[3], rect[2], 3), np.uint8(255))\n",
    "    sy = int((rect[3] - h) / 2)\n",
    "    sx = int((rect[2] - w) / 2)\n",
    "    s[sy:sy + h, sx:sx + w] = new_img\n",
    "    return s\n",
    "\n",
    "## Загрузка модели\n",
    "model = smp.FPN(encoder_name='resnet50', classes=24)\n",
    "model.load_state_dict(torch.load(\"./model_2\", map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "##Загрузка тестового изображения\n",
    "image = cv2.imread(f\"{os.environ['HOME']}/train/test.png\")\n",
    "converted_image = convert_image(image)\n",
    "## Вывод исходного изображения\n",
    "plt.subplot(211)\n",
    "im1 = plt.imshow(converted_image)\n",
    "\n",
    "## Применение сегментации\n",
    "transform = get_test_augmentation()\n",
    "image = transform(image=converted_image)['image']\n",
    "\n",
    "image = tensor_from_rgb_image(image)\n",
    "image = image.view((1, 3, image.shape[1], image.shape[2]))\n",
    "with torch.no_grad():\n",
    "    predict = model(image)\n",
    "predict = torch.sigmoid(predict) > 0.2\n",
    "res = mask_to_grayscale(predict[0])\n",
    "\n",
    "## Вывод результирующего изображения\n",
    "plt.subplot(212)\n",
    "im1 = plt.imshow(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in res:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os \n",
    "model = smp.FPN(encoder_name='resnet50', classes=24)\n",
    "model.load_state_dict(torch.load(\"./model_2\", map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "for filename in os.listdir('test_dataset_test'):\n",
    "    print(filename)\n",
    "    image = cv2.imread('test_dataset_test/'+filename)\n",
    "    converted_image = convert_image(image)\n",
    "    #plt.subplot(211)\n",
    "    #im1 = plt.imshow(converted_image)\n",
    "\n",
    "    ## Применение сегментации\n",
    "    transform = get_test_augmentation()\n",
    "    image = transform(image=converted_image)['image']\n",
    "\n",
    "    image = tensor_from_rgb_image(image)\n",
    "    image = image.view((1, 3, image.shape[1], image.shape[2]))\n",
    "    with torch.no_grad():\n",
    "        predict = model(image)\n",
    "    predict = torch.sigmoid(predict) > 0.2\n",
    "    res = mask_to_grayscale(predict[0])\n",
    "    im = Image.fromarray(res)\n",
    "    im.save(\"result/\"+filename)\n",
    "    ## Вывод результирующего изображения\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('result/'+'0a7fb4f3-7a13-466e-bed6-3a385a0eadc0.png')\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnIyPcTgle1F"
   },
   "source": [
    "Объявляем класс, используемый для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "umyG-IbclmCp"
   },
   "outputs": [],
   "source": [
    "class DetectionTrainer:\n",
    "\n",
    "    def __init__(self, model: nn.Module, optimizer: torch.optim.Optimizer,\n",
    "                 device: str, metric_functions: List[Tuple[str, Callable]] = [],\n",
    "                 callbacks: List[Callable[[nn.Module, int], None]] = [], epoch_number: int = 0):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.metric_functions = metric_functions\n",
    "        self.callbacks = callbacks\n",
    "\n",
    "        self.epoch_number = epoch_number\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate_batch(self, val_iterator: Iterator, eval_on_n_batches: int) -> Optional[Dict[str, float]]:\n",
    "        predictions = []\n",
    "        targets = []\n",
    "\n",
    "        for real_batch_number in range(eval_on_n_batches):\n",
    "            try:\n",
    "                xs, ys = next(val_iterator)\n",
    "\n",
    "                xs = torch.tensor(xs, device=self.device)\n",
    "                ys = [{key: torch.tensor(y[key], device=self.device) for key in y} for y in ys]\n",
    "            except StopIteration:\n",
    "                if real_batch_number == 0:\n",
    "                    return None\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            ys_pred = self.model.eval()(xs)\n",
    "\n",
    "            for y_pred, y_true in zip(ys_pred, ys):\n",
    "                predictions.append({key: y_pred[key].cpu().numpy() for key in y_pred})\n",
    "                targets.append({key: y_true[key].cpu().numpy() for key in y_true})\n",
    "\n",
    "        metrics = {}\n",
    "\n",
    "        predictions_tensor = []\n",
    "        targets_tensor = []\n",
    "\n",
    "        for y_pred, y_true in zip(predictions, targets):\n",
    "            predictions_tensor.append({key: torch.tensor(y_pred[key], device=self.device) for key in y_pred})\n",
    "            targets_tensor.append({key: torch.tensor(y_true[key], device=self.device) for key in y_true})\n",
    "\n",
    "        for metric_name, metric_fn in self.metric_functions:\n",
    "            metrics[metric_name] = metric_fn(predictions, targets).item()\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, val_loader: DataLoader, eval_on_n_batches: int = 1) -> Dict[str, float]:\n",
    "        metrics_sum = defaultdict(float)\n",
    "        num_batches = 0\n",
    "\n",
    "        val_iterator = iter(val_loader)\n",
    "\n",
    "        while True:\n",
    "            batch_metrics = self.evaluate_batch(val_iterator, eval_on_n_batches)\n",
    "\n",
    "            if batch_metrics is None:\n",
    "                break\n",
    "\n",
    "            for metric_name in batch_metrics:\n",
    "                metrics_sum[metric_name] += batch_metrics[metric_name]\n",
    "\n",
    "            num_batches += 1\n",
    "\n",
    "        metrics = {}\n",
    "\n",
    "        for metric_name in metrics_sum:\n",
    "            metrics[metric_name] = metrics_sum[metric_name] / num_batches\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def fit_batch(self, train_iterator: Iterator, update_every_n_batches: int) -> Optional[Dict[str, float]]:\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        predictions = []\n",
    "        targets = []\n",
    "\n",
    "        for real_batch_number in range(update_every_n_batches):\n",
    "            try:\n",
    "                xs, ys = next(train_iterator)\n",
    "                xs = torch.tensor(xs, device=self.device)\n",
    "                ys = [{key: torch.tensor(y[key], device=self.device) for key in y} for y in ys]\n",
    "            except StopIteration:\n",
    "                if real_batch_number == 0:\n",
    "                    return None\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            loss = sum(self.model.train()(xs, ys).values())\n",
    "\n",
    "            (loss / update_every_n_batches).backward()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                ys_pred = self.model.eval()(xs)\n",
    "\n",
    "                for y_pred, y_true in zip(ys_pred, ys):\n",
    "                    predictions.append({key: y_pred[key].cpu().numpy() for key in y_pred})\n",
    "                    targets.append({key: y_true[key].cpu().numpy() for key in y_true})\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        metrics = {}\n",
    "\n",
    "        predictions_tensor = []\n",
    "        targets_tensor = []\n",
    "\n",
    "        for y_pred, y_true in zip(predictions, targets):\n",
    "            predictions_tensor.append({key: torch.tensor(y_pred[key], device=self.device) for key in y_pred})\n",
    "            targets_tensor.append({key: torch.tensor(y_true[key], device=self.device) for key in y_true})\n",
    "\n",
    "        for metric_name, metric_fn in self.metric_functions:\n",
    "            metrics[metric_name] = metric_fn(predictions, targets).item()\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def fit_epoch(self, train_loader: DataLoader, update_every_n_batches: int = 1) -> Dict[str, float]:\n",
    "        metrics_sum = defaultdict(float)\n",
    "        num_batches = 0\n",
    "\n",
    "        train_iterator = iter(train_loader)\n",
    "\n",
    "        while True:\n",
    "            batch_metrics = self.fit_batch(train_iterator, update_every_n_batches)\n",
    "\n",
    "            if batch_metrics is None:\n",
    "                break\n",
    "\n",
    "            for metric_name in batch_metrics:\n",
    "                metrics_sum[metric_name] += batch_metrics[metric_name]\n",
    "\n",
    "            num_batches += 1\n",
    "\n",
    "        metrics = {}\n",
    "\n",
    "        for metric_name in metrics_sum:\n",
    "            metrics[metric_name] = metrics_sum[metric_name] / num_batches\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def fit(self, train_loader: DataLoader, num_epochs: int,\n",
    "            val_loader: DataLoader = None, update_every_n_batches: int = 1\n",
    "            ) -> Dict[str, np.ndarray]:\n",
    "        summary = defaultdict(list)\n",
    "\n",
    "        def save_metrics(metrics: Dict[str, float], postfix: str = '') -> None:\n",
    "            nonlocal summary, self\n",
    "\n",
    "            for metric in metrics:\n",
    "                metric_name, metric_value = f'{metric}{postfix}', metrics[metric]\n",
    "\n",
    "                summary[metric_name].append(metric_value)\n",
    "\n",
    "                print(f\"{metric_name}: {metric_value}, Epoch: {self.epoch_number}\")\n",
    "\n",
    "        for _ in tqdm(range(num_epochs - self.epoch_number), initial=self.epoch_number, total=num_epochs):\n",
    "            self.epoch_number += 1\n",
    "\n",
    "            train_metrics = self.fit_epoch(train_loader, update_every_n_batches)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                save_metrics(train_metrics, postfix='_train')\n",
    "\n",
    "                if val_loader is not None:\n",
    "                    test_metrics = self.evaluate(val_loader)\n",
    "                    save_metrics(test_metrics, postfix='_test')\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for callback in self.callbacks:\n",
    "                        callback(self.model, self.epoch_number)\n",
    "\n",
    "        summary = {metric: np.array(summary[metric]) for metric in summary}\n",
    "\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klMp7gYWmDck"
   },
   "source": [
    "Инициализация модели обучения и функций для подсчета метрик качества обучения нейросетей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-0D06LoHmQP2"
   },
   "outputs": [],
   "source": [
    "def initialize_model(num_classes: int,\n",
    "                     min_size: int, max_size: int,\n",
    "                     image_mean: Sequence[float], image_std: Sequence[float],\n",
    "                     device: str = None) -> torch.nn.Module:\n",
    "    pretrained_model = fasterrcnn_resnet50_fpn(pretrained=False, min_size=min_size, max_size=max_size,\n",
    "                                               image_mean=image_mean, image_std=image_std)\n",
    "    num_predictor_features = pretrained_model.roi_heads.box_head.fc7.out_features\n",
    "    pretrained_model.roi_heads.box_predictor = FastRCNNPredictor(num_predictor_features, num_classes)\n",
    "\n",
    "    return pretrained_model.to(device)\n",
    "\n",
    "\n",
    "def compute_iou(box1: torch.tensor, box2: torch.tensor) -> torch.tensor:\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "\n",
    "    if x2 - x1 < 0 or y2 - y1 < 0:\n",
    "        return 0\n",
    "\n",
    "    intersection = (x2 - x1) * (y2 - y1)\n",
    "    sum_ = ((box1[2] - box1[0]) * (box1[3] - box1[1]) +\n",
    "            (box2[2] - box2[0]) * (box2[3] - box2[1]))\n",
    "\n",
    "    iou = intersection / (sum_ - intersection)\n",
    "\n",
    "    return iou.item()\n",
    "\n",
    "\n",
    "class MeanAveragePrecision:\n",
    "\n",
    "    def __init__(self, num_conf: int = 11):\n",
    "        self.num_conf = num_conf\n",
    "\n",
    "    @staticmethod\n",
    "    def is_true(box: torch.tensor, label: int,\n",
    "                y_true: Dict[str, torch.tensor],\n",
    "                iou_thresh: float = 0.5) -> bool:\n",
    "        num_true = len(y_true['boxes'])\n",
    "\n",
    "        for i in range(num_true):\n",
    "            if label == y_true['labels'][i]:\n",
    "                if compute_iou(box, y_true['boxes'][i]) > iou_thresh:\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_map_given_conf(y_preds: List[Dict[str, torch.tensor]],\n",
    "                               y_trues: List[Dict[str, torch.tensor]],\n",
    "                               conf: float) -> float:\n",
    "        num_pos = defaultdict(int)\n",
    "        num_true_pos = defaultdict(int)\n",
    "\n",
    "        for y_pred, y_true in zip(y_preds, y_trues):\n",
    "\n",
    "            num_pred = len(y_pred['boxes'])\n",
    "\n",
    "            for i in range(num_pred):\n",
    "                if y_pred['scores'][i] > conf:\n",
    "                    label = y_pred['labels'][i]\n",
    "                    num_pos[label] += 1\n",
    "\n",
    "                    if MeanAveragePrecision.is_true(y_pred['boxes'][i], y_pred['labels'][i], y_true):\n",
    "                        num_true_pos[label] += 1\n",
    "\n",
    "        all_classes_sum = sum(num_true_pos[lbl] / num_pos[lbl] for lbl in num_pos)\n",
    "\n",
    "        try:\n",
    "            precision = all_classes_sum / len(num_pos)\n",
    "        except ZeroDivisionError:\n",
    "            precision = 0\n",
    "\n",
    "        return precision\n",
    "\n",
    "    def __call__(self, y_pred: List[Dict[str, torch.tensor]],\n",
    "                 y_true: List[Dict[str, torch.tensor]]) -> float:\n",
    "        assert len(y_pred) == len(y_true)\n",
    "\n",
    "        map_sum = 0\n",
    "\n",
    "        for conf in np.linspace(0, 1, self.num_conf):\n",
    "            map_sum += self.compute_map_given_conf(y_pred, y_true, conf)\n",
    "\n",
    "        return torch.tensor(map_sum / self.num_conf)\n",
    "\n",
    "\n",
    "def minimum_bounding_box(points: List[List[float]]) -> Tuple[float, float, float, float]:\n",
    "    x_min = min(p[0] for p in points)\n",
    "    y_min = min(p[1] for p in points)\n",
    "    x_max = max(p[0] for p in points)\n",
    "    y_max = max(p[1] for p in points)\n",
    "\n",
    "    return x_min, y_min, x_max, y_max\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VH_0TWXmYS0"
   },
   "source": [
    "Классы, для формирования датасетов(для обучения и для тестирования)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "esvFKW94mdaS"
   },
   "outputs": [],
   "source": [
    "class DataRetriever:\n",
    "\n",
    "    def __init__(self,\n",
    "                 dataset_path: str):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dataset_path = dataset_path\n",
    "        self.samples_paths = self.load_paths()\n",
    "        print(f'Found {len(self.samples_paths)} samples')\n",
    "\n",
    "    def remove_path(self, idx):\n",
    "        del self.samples_paths[idx]\n",
    "\n",
    "    def load_paths(self) -> List[str]:\n",
    "        samples_paths = []\n",
    "\n",
    "        for object_ in tqdm(os.listdir(self.dataset_path)):\n",
    "            ext = object_.split(\".\")[-1]\n",
    "\n",
    "            if ext != 'json':\n",
    "                continue\n",
    "\n",
    "            samples_paths.append(self.dataset_path + \"/\" + object_)\n",
    "\n",
    "        return samples_paths\n",
    "\n",
    "    def num_paths(self) -> int:\n",
    "        return len(self.samples_paths)\n",
    "\n",
    "\n",
    "class PLRCDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_retriever,\n",
    "                 class_ids: Dict,\n",
    "                 transform: A.BasicTransform = None,\n",
    "                 remove_unannotated: bool = False):\n",
    "        self.data_retriever = data_retriever\n",
    "\n",
    "        self.class_ids = class_ids\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        if remove_unannotated:\n",
    "            print(\"Removing unannotated samples\")\n",
    "            num_removed = self.remove_unannotated_paths()\n",
    "            print(f\"Removed {num_removed} unannotated samples\")\n",
    "\n",
    "    def make_layout(self, shapes: List[Dict], image_size: Tuple[int, int]) -> Dict[str, np.ndarray]:\n",
    "        shapes = self.select_shapes(shapes)\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        for shape in shapes:\n",
    "            if len(shape['points']) < 2:\n",
    "                continue\n",
    "\n",
    "            label = shape['label']\n",
    "\n",
    "            if label not in self.class_ids:\n",
    "                continue\n",
    "\n",
    "            x1, y1, x2, y2 = minimum_bounding_box(shape['points'])\n",
    "\n",
    "            x1 = max(x1, 0)\n",
    "            y1 = max(y1, 0)\n",
    "            x2 = min(x2, image_size[1] - 1)\n",
    "            y2 = min(y2, image_size[0] - 1)\n",
    "\n",
    "            if x1 == x2 or y1 == y2:\n",
    "                continue\n",
    "\n",
    "            boxes.append((x1, y1, x2, y2))\n",
    "            labels.append(self.class_ids[label])\n",
    "\n",
    "        if len(boxes) > 0:\n",
    "            boxes = np.array(boxes)\n",
    "            labels = np.array(labels, dtype=np.int64)\n",
    "        else:\n",
    "            boxes = np.empty((0, 4))\n",
    "            labels = np.empty(0, dtype=np.int64)\n",
    "\n",
    "        return {'boxes': boxes, 'labels': labels}\n",
    "\n",
    "    @staticmethod\n",
    "    def select_shapes(shapes: List[Dict]) -> List[Dict]:\n",
    "        good_shapes = []\n",
    "\n",
    "        for shape in shapes:\n",
    "            if shape['shape_type'] not in {'polygon', 'rectangle'} or len(shape['points']) < 2:\n",
    "                continue\n",
    "\n",
    "            good_shapes.append(shape)\n",
    "\n",
    "        return good_shapes\n",
    "\n",
    "    def remove_unannotated_paths(self) -> int:\n",
    "        counter = 0\n",
    "\n",
    "        for i in tqdm(range(len(self) - 1, -1, -1)):\n",
    "            _, y = self[i]\n",
    "            if len(y['boxes']) == 0:\n",
    "                self.data_retriever.remove_path(i)\n",
    "                counter += 1\n",
    "\n",
    "        return counter\n",
    "\n",
    "    @staticmethod\n",
    "    def decode_image(encoded_image: str) -> np.ndarray:\n",
    "        bytearray_ = np.asarray(bytearray(base64.b64decode(encoded_image)), dtype=np.uint8)\n",
    "        return cv2.imdecode(bytearray_, cv2.IMREAD_COLOR).astype(np.float32) / 255\n",
    "\n",
    "    def read_sample(self, json_path) -> Tuple[np.ndarray, Dict[str, np.ndarray]]:\n",
    "        with open(json_path, 'r') as f:\n",
    "            json_contents = json.load(f)\n",
    "\n",
    "        image = self.decode_image(json_contents['imageData'])\n",
    "        layout = self.make_layout(json_contents['shapes'], image.shape[:2])\n",
    "\n",
    "        return image, layout\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_path = self.data_retriever.samples_paths[idx]\n",
    "\n",
    "        x, y = self.read_sample(sample_path)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(image=x, bboxes=y['boxes'], labels=y['labels'])\n",
    "            x, y['boxes'], y['labels'] = transformed['image'], transformed['bboxes'], transformed['labels']\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_retriever.num_paths()\n",
    "\n",
    "\n",
    "class DatasetPart(Dataset):\n",
    "\n",
    "    def __init__(self, dataset: Dataset,\n",
    "                 indices: np.ndarray,\n",
    "                 transform: A.BasicTransform = None):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Any:\n",
    "        x, y = self.dataset[self.indices[idx]]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(image=x, bboxes=y['boxes'], labels=y['labels'])\n",
    "            x, y['boxes'], y['labels'] = transformed['image'], transformed['bboxes'], transformed['labels']\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HfkIFx1ylP7Q"
   },
   "source": [
    "Объявляем конфиг для object_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MCjuuZnqlXs3"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"dataset_path\": \"train/object_detection\",\n",
    "  \"model_log_interval\": 1,\n",
    "  \"data\": {\n",
    "    \"class_ids\": {\n",
    "      \"door\": 1\n",
    "    }\n",
    "  },\n",
    "  \"model\": {\n",
    "    \"image_size\": 512\n",
    "  },\n",
    "  \"training\": {\n",
    "    \"num_epochs\": 2,\n",
    "    \"batch_size\": 1,\n",
    "    \"update_every_n_batches\": 1,\n",
    "    \"device\": \"cpu\",\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"num_workers\": 0\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAss44LcmkvA"
   },
   "source": [
    "Код обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373,
     "referenced_widgets": [
      "1f6a44d536d84731a82ac6871085dbd2",
      "f67da5c894bd4941b4c2b693dd067636",
      "fb108adb8d00410e980b7d1e66b44262",
      "df2ec3c39e924d2394020b525b598536",
      "bc2bd476a44d427fb03d86ac038a2d69",
      "b77f1d736361484498345e29c41962b7",
      "0b8a886270204320ab131200ffd3878f",
      "b0cb6780a3324eaab6415a32a45d4f69",
      "53485f63532b4813a25e7d5ba9fa77e1",
      "39ac5c507a6549ee96e4aaf67908ddd1",
      "12f2787c151d43d2a15c3aacafaaac97"
     ]
    },
    "id": "ieEUG7eQmmQr",
    "outputId": "e2a17457-2e02-4869-8700-1bd7623bb713"
   },
   "outputs": [],
   "source": [
    "def make_transforms(config: Dict[str, Any]) -> Dict[str, Callable]:\n",
    "    size = config['model']['image_size']\n",
    "\n",
    "    train_list = [A.LongestMaxSize(size),\n",
    "                  A.PadIfNeeded(size, size, border_mode=cv2.BORDER_REPLICATE),\n",
    "                  ToTensorV2()]\n",
    "    eval_list = [A.LongestMaxSize(size),\n",
    "                 A.PadIfNeeded(size, size, border_mode=cv2.BORDER_REPLICATE),\n",
    "                 ToTensorV2()]\n",
    "\n",
    "    return {'train': A.Compose(train_list, bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'])),\n",
    "            'test': A.Compose(eval_list, bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))}\n",
    "\n",
    "\n",
    "def make_datasets(transforms, config: Dict[str, Any]) -> Dict[str, Dataset]:\n",
    "    all_data_retriever = DataRetriever(f\"{os.environ['HOME']}/{config['dataset_path']}\")\n",
    "\n",
    "    all_data_dataset = PLRCDataset(all_data_retriever, config['data']['class_ids'], remove_unannotated=True)\n",
    "\n",
    "    train_indices, test_indices = train_test_split(range(len(all_data_dataset)), test_size=0.3)\n",
    "\n",
    "    return {'train': DatasetPart(all_data_dataset, train_indices, transform=transforms['train']),\n",
    "            'test': DatasetPart(all_data_dataset, test_indices, transform=transforms['test'])}\n",
    "\n",
    "\n",
    "def make_loaders(datasets, config: Dict[str, Any]):\n",
    "    def collate_fn(samples):\n",
    "        xs, ys = list(zip(*samples))\n",
    "\n",
    "        for y in ys:\n",
    "            if len(y['boxes']) == 0:\n",
    "                y['boxes'] = np.empty((0, 4), dtype=np.float32)\n",
    "            else:\n",
    "                y['boxes'] = np.array(y['boxes'], dtype=np.float32)\n",
    "\n",
    "            y['labels'] = np.array(y['labels'], dtype=np.int64)\n",
    "\n",
    "        return torch.stack(xs), ys\n",
    "\n",
    "    loaders = {}\n",
    "\n",
    "    for name in datasets:\n",
    "        loaders[name] = DataLoader(datasets[name], config['training']['batch_size'],\n",
    "                                   num_workers=config['training']['num_workers'],\n",
    "                                   shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    return loaders\n",
    "\n",
    "\n",
    "def make_save_model_callback(log_interval: int):\n",
    "    def save_model(model, epoch):\n",
    "        if epoch % log_interval != 0:\n",
    "            return\n",
    "        torch.save(model.state_dict(), f\"./model_obj_{epoch}\")\n",
    "\n",
    "    return save_model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    transforms = make_transforms(config)\n",
    "    datasets = make_datasets(transforms, config)\n",
    "    data_loaders = make_loaders(datasets, config)\n",
    "    \n",
    "    model = initialize_model(1 + max(config['data']['class_ids'].values()),\n",
    "                             config['model']['image_size'], config['model']['image_size'], [0, 0, 0], [1, 1, 1],\n",
    "                             config['training']['device'])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), config['training']['learning_rate'])\n",
    "\n",
    "    metrics = [('mAP', MeanAveragePrecision())]\n",
    "\n",
    "    trainer = DetectionTrainer(model, optimizer, metric_functions=metrics,\n",
    "                               callbacks=[make_save_model_callback(config['model_log_interval'])],\n",
    "                               device=config['training']['device'])\n",
    "\n",
    "    trainer.fit(data_loaders['train'], config['training']['num_epochs'], val_loader=data_loaders['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ElCb7TxqoOYD"
   },
   "source": [
    "Вывод конечного результата"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "eQn_ToywouNC",
    "outputId": "e2c37fcc-e527-4eef-e6da-311f0e73abaf"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def draw_with_boxes(image: np.ndarray,\n",
    "                    preds: Dict[str, np.ndarray] = None,\n",
    "                    conf_threshold=0.5) -> np.ndarray:\n",
    "\n",
    "    image = image.copy()\n",
    "\n",
    "    font = cv2.FONT_HERSHEY_COMPLEX_SMALL\n",
    "    font_scale = 0.7\n",
    "    line_type = 1\n",
    "    label_padding = 5\n",
    "\n",
    "    num_boxes = len(preds['boxes'])\n",
    "\n",
    "    for i in range(num_boxes):\n",
    "        if preds['scores'][i] < conf_threshold:\n",
    "            continue\n",
    "        x_min, y_min, x_max, y_max = preds['boxes'][i]\n",
    "\n",
    "        cv2.rectangle(img=image,\n",
    "                      pt1=(int(x_min), int(y_min)),\n",
    "                      pt2=(int(x_max), int(y_max)),\n",
    "                      color=(0, 0, 255),\n",
    "                      thickness=-1)\n",
    "\n",
    "    return image\n",
    "\n",
    "##Загрузка тестового изображения\n",
    "image = cv2.imread(f\"{os.environ['HOME']}/train/test.png\").astype(np.float32) / 255\n",
    "\n",
    "## Вывод исходного изображения\n",
    "plt.subplot(211)\n",
    "im1 = plt.imshow(image)\n",
    "\n",
    "## Применение модели\n",
    "class_ids = config['data']['class_ids']\n",
    "reverse_classes_map = {v: k for k, v in class_ids.items()}\n",
    "bytearray_ = np.asarray(bytearray(image), dtype=np.uint8)\n",
    "x = image\n",
    "transform = ToTensorV2()\n",
    "model = initialize_model(1 + max(config['data']['class_ids'].values()),\n",
    "                             config['model']['image_size'], config['model']['image_size'], [0, 0, 0], [1, 1, 1],\n",
    "                             config['training']['device'])\n",
    "model.load_state_dict(torch.load(\"./model_obj_2\"))\n",
    "x = transform(image=x)['image']\n",
    "with torch.no_grad():\n",
    "  y_pred = model.eval()([torch.tensor(x, device=config['training']['device'])])[0]\n",
    "  y_pred = {key: y_pred[key].cpu().numpy() for key in y_pred}\n",
    "  y_pred['labels'] = np.array([reverse_classes_map[label] for label in y_pred['labels']])\n",
    "\n",
    "  x = torch.moveaxis(x, 0, -1).cpu().numpy()\n",
    "  x = np.ascontiguousarray(255 * x, dtype=np.uint8)\n",
    "\n",
    "image_with_boxes = draw_with_boxes(x, y_pred)\n",
    "\n",
    "## Вывод реузультирующего изображения\n",
    "plt.subplot(212)\n",
    "im1 = plt.imshow(image_with_boxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0b8a886270204320ab131200ffd3878f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "12f2787c151d43d2a15c3aacafaaac97": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1f6a44d536d84731a82ac6871085dbd2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f67da5c894bd4941b4c2b693dd067636",
       "IPY_MODEL_fb108adb8d00410e980b7d1e66b44262",
       "IPY_MODEL_df2ec3c39e924d2394020b525b598536"
      ],
      "layout": "IPY_MODEL_bc2bd476a44d427fb03d86ac038a2d69"
     }
    },
    "39ac5c507a6549ee96e4aaf67908ddd1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "53485f63532b4813a25e7d5ba9fa77e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b0cb6780a3324eaab6415a32a45d4f69": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b77f1d736361484498345e29c41962b7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bc2bd476a44d427fb03d86ac038a2d69": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "df2ec3c39e924d2394020b525b598536": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_39ac5c507a6549ee96e4aaf67908ddd1",
      "placeholder": "​",
      "style": "IPY_MODEL_12f2787c151d43d2a15c3aacafaaac97",
      "value": " 97.8M/97.8M [00:01&lt;00:00, 111MB/s]"
     }
    },
    "f67da5c894bd4941b4c2b693dd067636": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b77f1d736361484498345e29c41962b7",
      "placeholder": "​",
      "style": "IPY_MODEL_0b8a886270204320ab131200ffd3878f",
      "value": "100%"
     }
    },
    "fb108adb8d00410e980b7d1e66b44262": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b0cb6780a3324eaab6415a32a45d4f69",
      "max": 102530333,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_53485f63532b4813a25e7d5ba9fa77e1",
      "value": 102530333
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
